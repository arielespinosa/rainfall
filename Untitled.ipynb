{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from preprocess.files import files_list, read_serialize_file, write_serialize_file\n",
    "from config import *\n",
    "import numpy as np\n",
    "import keras\n",
    "import random as rnd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TerminateOnNaN\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from keras_models import MultiLayerPerceptron, SVM, Long_Short_Term_Memory\n",
    "import tensorflow as tf\n",
    "from visualization.plot_models import TrainingPlot\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_model() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-472b410c749a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# ------ Long Short-Term Memory ------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLong_Short_Term_Memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ariel/rain/proyecto2/keras_models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_generator, validation_generator, workers, use_multiprocessing, epochs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ariel/rain/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ariel/rain/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ariel/rain/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     callbacks.set_params({\n\u001b[1;32m     96\u001b[0m         \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Ariel/rain/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_model() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "path = os.path.join(BASE_DIR, \"rna/logs/\")\n",
    "\n",
    "parameters_svm = {\n",
    "    # First value means the one to one input layer-hidden layer conection.\n",
    "    # Array values is for fully connected dense hidden layer\n",
    "    \"dense_units\"   : (450, [600, 500]),\n",
    "    \"h_activation\"  : \"relu\",\n",
    "    \"o_activation\"  : \"softmax\",\n",
    "    # (momentum, epsilon)\n",
    "    \"batch_norm\"    : [(0.99, 0.001), (0.99, 0.001), (0.99, 0.001)],\n",
    "    \"dropout\"       : \"d\",\n",
    "    \"dropout_rate\"  : [0.3, 0.25, 0.2],\n",
    "    \"optimizer\"     : SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "    \"loss\"          : \"mse\",\n",
    "    \"metrics\"       : [\"accuracy\", \"mse\", \"mae\"],\n",
    "    \"shape\"         : (375, ),\n",
    "    \"callbacks\"     : None, #[EarlyStopping(monitor = \"val_loss\", patience = 2), TensorBoard(log_dir = path, write_graph = True, write_images = True, histogram_freq = 3), TerminateOnNaN()],\n",
    "    \"name\"          : \"svm_model\",\n",
    "    \"kernel_initializer\" : None,\n",
    "}\n",
    "\n",
    "parameters_mlp = {\n",
    "    # A tuple containing number of neurons for each hidden layer:\n",
    "    # (for single layer you can use a scalar specifing number of neurons)\n",
    "    # dense_units = shape + shape/3 - 500\n",
    "    \"dense_units\"   : [1600, 800, 500],\n",
    "    # non-linear activation for hidden layers: [linear, sigmoid, tanh, softmax, relu]\n",
    "    \"h_activation\"  : \"relu\",\n",
    "    # non-linear activation for output layer:\n",
    "    # (depends on the output variable)\n",
    "    \"o_activation\"  : \"sigmoid\",\n",
    "    # Batch-normalization between layers:\n",
    "    # (use for deep networks, helps with convergence and overfitting)\n",
    "    \"batch_norm\"    : [(0.99, 0.001), (0.99, 0.001), (0.99, 0.001)],\n",
    "    # Apply dropout to dense layers to avoid overfitting: \n",
    "    # Dropout = d,\n",
    "    # AlphaDropout = ad\n",
    "    # (use with high number of hidden neurons)\n",
    "    \"dropout\"       : \"d\",\n",
    "    # fraction of neurons to shot down when dropout is activated:\n",
    "    \"dropout_rate\"  : [0.3, 0.25, 0.2],\n",
    "    # Optimizer to use for trainning the network: [Adam, Adadelta, RMSprop]\n",
    "    \"optimizer\"     : SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True),\n",
    "    # Cost function to minimize during trainning: ['mse', 'mae', 'msle']\n",
    "    # (for output variables in range (0, 1) use 'binary_crossentropy',\n",
    "    # for classification problems use: 'categorical_crossentropy')    \n",
    "    \"loss\"          : \"mse\",\n",
    "    \"metrics\"       : [\"accuracy\", \"mse\", \"mae\"],\n",
    "    # input shape. Flatten is 1125 neurons\n",
    "    \"shape\"         : (375, 3),\n",
    "    \"name\"          :\"mlp_model\",\n",
    "    \"callbacks\"     : [EarlyStopping(monitor = \"val_loss\", patience=2), \n",
    "                       TensorBoard(log_dir = path, write_graph = True, write_images = True), \n",
    "                       TerminateOnNaN()],\n",
    "    # distribution for random initial state: ['glorot_uniform', 'lecun_normal']\n",
    "    \"kernel_initializer\" : \"\",\n",
    "}\n",
    "\n",
    "parameters_lstm = {\n",
    "    # Array values is for fully connected dense hidden layer\n",
    "    \"lstm_units\"       : [800, 600],  \n",
    "    \"stateful\"         : [True, False], \n",
    "    \"return_sequences\" : [True, False],\n",
    "    \"h_activation\"     : \"relu\",\n",
    "    \"o_activation\"     : \"softmax\",\n",
    "    \"optimizer\"        : SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "    \"loss\"             : \"mse\",\n",
    "    \"batch_shape\"      : (15, 375, 3),\n",
    "    \"metrics\"          : [\"mse\", \"mae\"],\n",
    "    \"callbacks\"        : [TrainingPlot], #[EarlyStopping(monitor='mse', patience=2), TensorBoard(log_dir=path, write_graph=True, write_images=False, histogram_freq=3), TerminateOnNaN()],\n",
    "    \"name\"             : \"lstm_model\",\n",
    "}\n",
    "\n",
    "# Load serialized data files ready for start to start train  proccess (OK)\n",
    "train_files   = files_list(DATASET_DIR)\n",
    "predict_files = files_list(PREDICT_DIR)\n",
    "\n",
    "# This is for not let the networks train with same dataset (OK)\n",
    "rnd.shuffle(train_files)\n",
    "\n",
    "training_generator   = DataGenerator(\"lstm\", train_files[:70], batch_size = 15, shuffle = False)\n",
    "validation_generator = DataGenerator(\"lstm\", train_files[70:75], batch_size = 15, shuffle = False)\n",
    "prediction_generator = DataGenerator(\"lstm\", predict_files, shuffle = False, train = False)\n",
    "\n",
    "#training_generator   = NSVM_DataGenerator(files[:7000], batch_size = 500)\n",
    "#validation_generator = NSVM_DataGenerator(files[7000:7500], batch_size = 500)\n",
    "\n",
    "#training_generator   = LSTM_DataGenerator(files[:7000], batch_size = 25)00\n",
    "#validation_generator = LSTM_DataGenerator(files[7000:7500], batch_size = 25)\n",
    "\n",
    "\n",
    "# Neuronal Networks Models ------------------------------------------------------------------------\n",
    "# ------ Suport Vector Machines ------ \n",
    "#svm = SVM(parameters_svm)\n",
    "#print(svm.model.summary())\n",
    "#svm_history = svm.train(training_generator, validation_generator, workers=20, use_multiprocessing=True, epochs=20)\n",
    "#print(svm_history.history)\n",
    "#svm.model.fit_generator(generator=training_generator, workers=20, use_multiprocessing=True, epochs=2)\n",
    "\n",
    "# ------ Multilayer Perceptron ------ (OK)\n",
    "# Create an MLP NN. Then start training and validation process. After that, make all predictions for predict dataset.\n",
    "# At the end model is save in rna/models/ path with his name defined in his parameters.\n",
    "\"\"\"\n",
    "mlp = MultiLayerPerceptron(parameters_mlp)\n",
    "mlp.train(training_generator, validation_generator, workers=20, use_multiprocessing=True, epochs=5)\n",
    "mlp.save()\n",
    "\"\"\"\n",
    "\n",
    "# ------ Long Short-Term Memory ------ \n",
    "lstm = Long_Short_Term_Memory(parameters_lstm)\n",
    "lstm.train(training_generator, validation_generator, workers=20, use_multiprocessing=True, epochs=5)\n",
    "lstm.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
